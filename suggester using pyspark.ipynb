{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Restaurant recommendation System\n",
    "\n",
    "\n",
    "This is an extended version of my restaurnt suggester project <a href=''>(view the raw files here)</a> and <a href=''>(site over here) </a> \n",
    "\n",
    "<br>\n",
    "In This project I have created a recommender function based of TF-IDF vectorizer and makes recommendations based of the content. I have also tried building a Tensorflow based recommender model which recommends the restaurnts. I have also tried K-means Algorithm and ALS Algorithm. I have described the steps that is involed in the code cells below. The diagram below depects the entire process of recommending restaurnts based of differrent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark=SparkSession.builder.appName(\"suggester\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('cleaned_file.csv', header='true', inferSchema='false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d6762a-d5b0-45a1-8649-11f4fad30f76",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Unnamed: 0: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- online_order: string (nullable = true)\n",
      " |-- book_table: string (nullable = true)\n",
      " |-- rate: string (nullable = true)\n",
      " |-- votes: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- rest_type: string (nullable = true)\n",
      " |-- dish_liked: string (nullable = true)\n",
      " |-- cuisines: string (nullable = true)\n",
      " |-- cost: string (nullable = true)\n",
      " |-- reviews_list: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- cost_category: string (nullable = true)\n",
      " |-- bag_of_words: string (nullable = true)\n",
      " |-- lemmatized_reviews: string (nullable = true)\n",
      "\n",
      "+---------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+------+--------------------+--------------------+--------------------+\n",
      "|     Unnamed: 0|                 url|             address|                name|        online_order|book_table|                rate| votes|               phone|            location|           rest_type|          dish_liked|            cuisines| cost|        reviews_list|  type|       cost_category|        bag_of_words|  lemmatized_reviews|\n",
      "+---------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+------+--------------------+--------------------+--------------------+\n",
      "|              0|https://www.zomat...|942, 21st Main Ro...|               Jalsa|                 Yes|       Yes|                 4.1| 775.0|        080 42297555|                NULL|                NULL|                NULL|                NULL| NULL|                NULL|  NULL|                NULL|                NULL|                NULL|\n",
      "|+91 9743772233\"|        Banashankari|       Casual Dining|Pasta, Lunch Buff...|North Indian, Mug...|     800.0|a beautiful place...|Buffet|med-range-restaurant|Casual Dining   a...|['a', 'beautiful'...|                NULL|                NULL| NULL|                NULL|  NULL|                NULL|                NULL|                NULL|\n",
      "|              1|https://www.zomat...|2nd Floor, 80 Fee...|      Spice Elephant|                 Yes|        No|                 4.1| 787.0|        080 41714161|        Banashankari|       Casual Dining|Momos, Lunch Buff...|Chinese, North In...|800.0|had been here for...|Buffet|med-range-restaurant|Casual Dining   h...|['had', 'been', '...|\n",
      "|              2|https://www.zomat...|1112, Next to KIM...|     San Churro Cafe|                 Yes|        No|                 3.8| 918.0|      +91 9663487993|        Banashankari| Cafe, Casual Dining|Churros, Cannello...|Cafe, Mexican, It...|800.0|ambience is not t...|Buffet|med-range-restaurant|Cafe, Casual Dini...|['ambience', 'is'...|\n",
      "|              3|https://www.zomat...|1st Floor, Annaku...|Addhuri Udupi Bho...|                  No|        No|                 3.7|  88.0|      +91 9620009302|        Banashankari|         Quick Bites|         Masala Dosa|South Indian, Nor...|300.0|great food and pr...|Buffet|med-range-restaurant|Quick Bites   gre...|['great', 'food',...|\n",
      "+---------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.columns\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_map = {\n",
    "    'cheap-eats': ('cheap', 'inexpensive', 'low-price', 'low-cost', 'economical',\n",
    "                   'economic', 'affordable'),\n",
    "    'mid-range': ('moderate', 'fair', 'mid-price', 'reasonable', 'average'),\n",
    "    'fine-dining': ('expensive', 'fancy', 'lavish')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'StopWordsRemover' has no attribute 'loadStopWords'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m process_text(text)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Create a StopWordsRemover object (optional, for additional stop word removal)\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m stop_words_remover \u001b[38;5;241m=\u001b[39m StopWordsRemover(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, stopWords\u001b[38;5;241m=\u001b[39m\u001b[43mStopWordsRemover\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadStopWords\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'StopWordsRemover' has no attribute 'loadStopWords'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "def process_sentences(text):\n",
    "    # Define a PySpark UDF (User Defined Function)\n",
    "    @udf(returnType=F.StringType())\n",
    "    def process_text(text):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")  # Load a small English language model\n",
    "        doc = nlp(text)\n",
    "\n",
    "    processed_tokens = []\n",
    "    for token in doc:\n",
    "      # Lemmatize based on part-of-speech tags (similar to NLTK approach)\n",
    "        if token.pos_ in (\"VERB\",):  # Only verbs\n",
    "            lemma = token.lemma_.lower()  # Lowercase the lemma\n",
    "        else:\n",
    "            lemma = token.lemma_.lower()\n",
    "\n",
    "      # Remove stop words and keep only alphanumeric characters\n",
    "        if lemma not in spacy.lang.en.stop_words and lemma.isalnum():\n",
    "            processed_tokens.append(lemma)\n",
    "\n",
    "    # Join processed tokens with spaces and perform some cleanup (similar to NLTK)\n",
    "    processed_text = \" \".join(processed_tokens)\n",
    "    processed_text = processed_text.replace(\"n't\", \" not\")\n",
    "    processed_text = processed_text.replace(\"'m\", \" am\")\n",
    "    processed_text = processed_text.replace(\"'s\", \" is\")\n",
    "    processed_text = processed_text.replace(\"'re\", \" are\")\n",
    "    processed_text = processed_text.replace(\"'ll\", \" will\")\n",
    "    processed_text = processed_text.replace(\"'ve\", \" have\")\n",
    "    processed_text = processed_text.replace(\"'d\", \" would\")\n",
    "    return processed_text\n",
    "\n",
    "    return process_text(text)\n",
    "\n",
    "# Create a StopWordsRemover object (optional, for additional stop word removal)\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"processed_text\", outputCol=\"clean_text\", stopWords=StopWordsRemover.loadStopWords(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, lower, explode, split, array_contains\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "def preprocessing(input_text):\n",
    "    # Fill missing values (consider replacing with your logic)\n",
    "    df = df.fillna(\"unknown\", [\"location\", \"cuisines\", \"dish_liked\", \"rest_type\", \"reviews_list\"])\n",
    "    # Price mapping UDF (assuming 'cost' column exists)\n",
    "    price_map_udf = udf(lambda x: next((v for k, v in price_map.items() if x.lower() in k), \"unknown\"), StringType())\n",
    "    df = df.withColumn(\"price_category\", price_map_udf(col(\"cost\")))\n",
    "\n",
    "    # Cost categorization UDF (assuming 'cost' column exists)\n",
    "    cost_categorizer_udf = udf(lambda x: \"cheap-eats restaurant\" if x <= 200 \n",
    "                               else (\"med-range-restaurant\" if x <= 800 else \"fine-dining restaurant\"), StringType())\n",
    "    df = df.withColumn(\"cost_category\", cost_categorizer_udf(col(\"cost\")))\n",
    "\n",
    "    # Define stop words and lemmatizer functions (UDFs)\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "    def lemmatizer_udf(text):\n",
    "        temp_sent = []\n",
    "        words = split(lower(text), \"\\\\W+\")  # Split on non-word characters\n",
    "        tags = nltk.pos_tag([w.rdd.first() for w in words.rdd])  # Get POS tags (assuming single partition)\n",
    "        for i, word in enumerate(words.rdd.collect()):\n",
    "            if tags[i][1] in (\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"):  # Only verbs\n",
    "                lemmatized = WordNetLemmatizer().lemmatize(word, pos=\"v\")\n",
    "            else:\n",
    "                lemmatized = WordNetLemmatizer().lemmatize(word)\n",
    "            if lemmatized not in stop_words and lemmatized.isalpha():\n",
    "                temp_sent.append(lemmatized)\n",
    "    full_sentence = \" \".join(temp_sent)\n",
    "    full_sentence = full_sentence.replace(\"n't\", \" not\")\n",
    "    # ... other replacements (same as original code)\n",
    "    return full_sentence\n",
    "\n",
    "    lemmatizer_udf = udf(lemmatizer_udf, StringType())\n",
    "\n",
    "    # Create a new column with processed reviews using lemmatizer UDF\n",
    "    df = df.withColumn(\"processed_reviews\", lemmatizer_udf(explode(split(col(\"reviews_list\"), \"\\\\|\"))))\n",
    "\n",
    "    # Combine relevant columns into a bag-of-words feature\n",
    "    assembler = VectorAssembler(inputCols=[\"rest_type\", \"processed_reviews\"], outputCol=\"bag_of_words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommends restaurants based on name and lemmatized reviews (CBF)\n",
    "<br>\n",
    "Args:\n",
    "<br>\n",
    "- user_name: Name of the restaurant for which to recommend similar ones \n",
    "<br>\n",
    "- data: Spark DataFrame containing restaurant data\n",
    "<br>\n",
    "- lemmatized_column: Column containing lemmatized reviews\n",
    "<br>\n",
    "- k: Number of recommendations to return\n",
    "<br>\n",
    "Returns:\n",
    "- list: List of recommended restaurant names (excluding the user's restaurant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e989706c-b145-4516-be2f-2b8a620b2a59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, explode, split, array_contains,split\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "#Calculates cosine similarity between two sparse vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return float(vec1.dot(vec2)) / (vec1.norm() * vec2.norm())\n",
    "cosine_similarity_udf = udf(cosine_similarity, FloatType())\n",
    "\n",
    "\n",
    "df = df.withColumn(\"review_list\", split(col(\"lemmatized_reviews\"), \"\\\\|\"))\n",
    "\n",
    "def recommend_restaurants(user_name, data, lemmatized_column='lemmatized_reviews', k=10):\n",
    "    # HashingTF for feature extraction\n",
    "    hashingTF = HashingTF(inputCol=lemmatized_column, outputCol=\"features\")\n",
    "\n",
    "    # IDF for weighting terms\n",
    "    idf = IDF(inputCol=\"features\", outputCol=\"weighted_features\")\n",
    "\n",
    "    # Feature engineering pipeline\n",
    "    assembler = Pipeline(stages=[hashingTF, idf])\n",
    "    data_with_features = assembler.fit(data).transform(data)\n",
    "\n",
    "    # Get user's review as a list of words\n",
    "    user_review = data.filter(col(\"name\") == user_name).select(lemmatized_column).rdd.flatMap(lambda x: x).first()\n",
    "\n",
    "    # Convert user review to a sparse vector\n",
    "    user_review_vector = Vectors.sparse(len(data.select(lemmatized_column).first()[0]), user_review)\n",
    "\n",
    "    # Find most similar restaurants (excluding user's restaurant)\n",
    "    top_k_similar = data_with_features.withColumn(\"similarity\", cosine_similarity_udf(user_review_vector, col(\"weighted_features\"))).filter(col(\"name\") != user_name).sort(col('similarity').desc()).limit(k)  # Replace \"id\" with your restaurant ID column\n",
    "\n",
    "    # Extract recommended restaurant names\n",
    "    recommended_restaurants = top_k_similar.select(\"name\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    return recommended_restaurants\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def recommend(input_text):\n",
    "    \"\"\"\n",
    "    Recommends restaurants based on user input using PySpark.\n",
    "\n",
    "    Args:\n",
    "      input_text: User's search query.\n",
    "\n",
    "    Returns:\n",
    "      str: HTML table containing top 10 recommended restaurants.\n",
    "    \"\"\"\n",
    "\n",
    "    # Preprocess user input text\n",
    "    processed_text = preprocessing(input_text)\n",
    "\n",
    "    # Filter data based on extracted criteria\n",
    "    filtered_data = df.where(\n",
    "      F.col(\"location\").isin(F.explode(F.split(processed_text, \" \"))).filter(F.col(\"location\").isNotNull()) &\n",
    "      F.col(\"cuisines\").isin(F.explode(F.split(processed_text, \" \"))).filter(F.col(\"cuisines\").isNotNull()) &\n",
    "      F.col(\"dish_liked\").isin(F.explode(F.split(processed_text, \" \"))).filter(F.col(\"dish_liked\").isNotNull()) &\n",
    "      F.col(\"rest_type\").isin(F.explode(F.split(processed_text, \" \"))).filter(F.col(\"rest_type\").isNotNull())\n",
    "    )\n",
    "\n",
    "    # Filter data based on price (assuming price_map is a dictionary)\n",
    "    if price_map:\n",
    "        for key, value in price_map.items():\n",
    "            if any(v in processed_text for v in value):\n",
    "                filtered_data = filtered_data.filter(F.col(\"price\") == key)\n",
    "                break\n",
    "\n",
    "    # TF-IDF vectorization\n",
    "    from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "    hashingTF = HashingTF(inputCol=\"bag_of_words\", outputCol=\"features\")\n",
    "    idf = IDF(inputCol=\"features\", outputCol=\"tfidf_features\")\n",
    "\n",
    "    featurizedData = hashingTF.transform(filtered_data)\n",
    "    idfModel = idf.fit(featurizedData)\n",
    "    rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    input_vector = Vectors.dense(rescaledData.select(\"tfidf_features\").first().toArray())\n",
    "    cosine_similarities = rescaledData.select(\n",
    "      col(\"name\"), col(\"location\"), col(\"cost\"), col(\"cuisines\"), col(\"reviews_list\"),\n",
    "      F.dot(col(\"tfidf_features\"), input_vector) / (F.vector_norm(col(\"tfidf_features\")) * F.vector_norm(input_vector))\n",
    "    )\n",
    "\n",
    "    # Sort and return recommendations\n",
    "    recommendations = cosine_similarities.sort(\n",
    "      F.col(\"dot(tfidf_features, input_vector) / (vector_norm(tfidf_features) * vector_norm(input_vector))\").desc()\n",
    "    ).select(\"name\", \"location\", \"cost\", \"cuisines\", \"reviews_list\").limit(10).toHtml(index=False)\n",
    "\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "preprocessing() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m user_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPizza Palace\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the restaurant name for recommendations\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mrecommend\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommendations for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m restaurant \u001b[38;5;129;01min\u001b[39;00m recommendations:\n",
      "Cell \u001b[1;32mIn[28], line 15\u001b[0m, in \u001b[0;36mrecommend\u001b[1;34m(input_text)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mRecommends restaurants based on user input using PySpark.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m  str: HTML table containing top 10 recommended restaurants.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Preprocess user input text\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m processed_text \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Filter data based on extracted criteria\u001b[39;00m\n\u001b[0;32m     18\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[0;32m     19\u001b[0m   F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misin(F\u001b[38;5;241m.\u001b[39mexplode(F\u001b[38;5;241m.\u001b[39msplit(processed_text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)))\u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNotNull()) \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m     20\u001b[0m   F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuisines\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misin(F\u001b[38;5;241m.\u001b[39mexplode(F\u001b[38;5;241m.\u001b[39msplit(processed_text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)))\u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuisines\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNotNull()) \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m     21\u001b[0m   F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdish_liked\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misin(F\u001b[38;5;241m.\u001b[39mexplode(F\u001b[38;5;241m.\u001b[39msplit(processed_text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)))\u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdish_liked\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNotNull()) \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m     22\u001b[0m   F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrest_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misin(F\u001b[38;5;241m.\u001b[39mexplode(F\u001b[38;5;241m.\u001b[39msplit(processed_text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)))\u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrest_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNotNull())\n\u001b[0;32m     23\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: preprocessing() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "user_name = \"Pizza Palace\"  # Replace with the restaurant name for recommendations\n",
    "recommendations = recommend(user_name)\n",
    "\n",
    "print(f\"Recommendations for {user_name}:\")\n",
    "for restaurant in recommendations:\n",
    "    print(restaurant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d134cae-3240-431f-be75-e3bab7d627f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, lower, collect_list, lit\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f7885f4-0dee-45ba-8fb0-a08ef6988310",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def recommend(input_text):\n",
    "    \"\"\"Recommends restaurants based on user input using PySpark.\"\"\"\n",
    "\n",
    "    # Lowercase input text\n",
    "    input_text = lower(input_text)\n",
    "\n",
    "    # Filter data based on extracted criteria\n",
    "    data = df.filter(\n",
    "        (col(\"location\").isin(extract_list(input_text, \"location\", df))) &\n",
    "        (col(\"cuisines\").isin(extract_list(input_text, \"cuisines\", df))) &\n",
    "        (col(\"dish_liked\").isin(extract_list(input_text, \"dish_liked\", df))) &\n",
    "        (col(\"rest_type\").isin(extract_list(input_text, \"rest_type\", df))) &\n",
    "        #(col(\"price\") == extract_price(input_text, price_map))\n",
    "    )\n",
    "\n",
    "    # Process user description text input\n",
    "    processed_input = process_sentences(input_text)\n",
    "\n",
    "    # TF-IDF vectorization\n",
    "    hashingTF = HashingTF(inputCol=\"bag_of_words\", outputCol=\"features\")\n",
    "    featurizedData = hashingTF.transform(data)\n",
    "    idf = IDF(inputCol=\"features\", outputCol=\"tfidf_features\")\n",
    "    idfModel = idf.fit(featurizedData)\n",
    "    rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    input_vector = Vectors.dense(rescaledData.select(\"tfidf_features\").first().toArray())\n",
    "    cosine_similarities = rescaledData.select(col(\"name\"), col(\"location\"), col(\"cost\"), col(\"cuisines\"), col(\"reviews_list\"), dot(col(\"tfidf_features\"), input_vector) / (vector_norm(col(\"tfidf_features\")) * vector_norm(input_vector)))\n",
    "\n",
    "    # Sort and return recommendations\n",
    "    recommendations = cosine_similarities.sort(col(\"dot(tfidf_features, input_vector) / (vector_norm(tfidf_features) * vector_norm(input_vector))\").desc()) \\\n",
    "                                            .select(\"name\", \"location\", \"cost\", \"cuisines\", \"reviews_list\") \\\n",
    "                                            .toPandas() \\\n",
    "                                            .to_html(index=False)\n",
    "    return recommendations\n",
    "\n",
    "# Helper functions for extracting lists and price\n",
    "extract_list_udf = udf(extract_list, ArrayType(StringType()))\n",
    "#extract_price_udf = udf(extract_price, StringType())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1de944a-7e04-4f1d-ae92-a2562641c5ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Assuming you have a SparkSession named 'spark' and a DataFrame named 'data'\n",
    "# with user_id, restaurant_id, and rating columns\n",
    "\n",
    "# Prepare user-item interaction matrix\n",
    "user_item_matrix = df.groupBy(\"name\", \"restaurant_id\").agg(\n",
    "    sum(\"rating\").alias(\"rating\")\n",
    ")\n",
    "\n",
    "# Train the ALS model\n",
    "als = ALS(rank=10, maxIter=10, regParam=0.1, userCol=\"user_id\", itemCol=\"restaurant_id\", ratingCol=\"rating\")\n",
    "model = als.fit(user_item_matrix)\n",
    "\n",
    "# Get user profile vector for a specific user\n",
    "user_id = 123  # Replace with desired user ID\n",
    "user_vector = model.userFactors.filter(col(\"id\") == user_id).select(\"features\").first()\n",
    "\n",
    "# Recommend top-K restaurants for the user\n",
    "top_k_recs = model.recommendForUser(user_id, 10)  # Recommend 10 restaurants\n",
    "recommendations = top_k_recs.join(data.select(\"restaurant_id\", \"name\"), on=\"restaurant_id\") \\\n",
    "                            .select(\"name\", \"rating\") \\\n",
    "                            .orderBy(col(\"rating\").desc()) \\\n",
    "                            .toPandas()\n",
    "\n",
    "print(f\"Top 10 Recommendations for User {user_id}:\")\n",
    "print(recommendations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23e0a61e-814d-4f2e-b884-c3e74cf48823",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Assuming you have a SparkSession named 'spark' and a DataFrame named 'data'\n",
    "# with 'latitude' and 'longitude' columns for restaurant locations\n",
    "\n",
    "# Create a feature vector for latitude and longitude\n",
    "assembler = VectorAssembler(inputCols=[\"latitude\", \"longitude\"], outputCol=\"features\")\n",
    "data_with_features = assembler.transform(data)\n",
    "\n",
    "# Define the number of clusters (k) based on your analysis\n",
    "num_clusters = 4  # Adjust this value as needed\n",
    "\n",
    "# Train the K-Means model\n",
    "kmeans = KMeans(k=num_clusters, seed=1)  # Set a random seed for reproducibility\n",
    "model = kmeans.fit(data_with_features)\n",
    "\n",
    "# Predict cluster labels for each restaurant\n",
    "predictions = model.transform(data_with_features)\n",
    "restaurants_with_cluster = predictions.select(\"name\", \"latitude\", \"longitude\", \"prediction\")  # Assuming a 'name' column\n",
    "\n",
    "# Analyze the clusters (optional)\n",
    "# You can explore the centroids (cluster centers) and visualize the clusters on a map\n",
    "centroids = model.clusterCenters()\n",
    "print(\"Cluster centers:\")\n",
    "centroids.foreach(lambda row: print(row))\n",
    "\n",
    "print(\"Sample restaurants with assigned clusters:\")\n",
    "restaurants_with_cluster.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0812999-0534-4239-990c-417d3aa8ed70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dot, Dense\n",
    "\n",
    "# User and Restaurant Embedding Layers (dimensions to be chosen)\n",
    "user_embedding = Embedding(num_users, embedding_dim)\n",
    "restaurant_embedding = Embedding(num_restaurants, embedding_dim)\n",
    "\n",
    "# Additional Embedding Layers for other features (if applicable)\n",
    "\n",
    "# User-Restaurant Interaction Layer (e.g., dot product)\n",
    "user_restaurant_interaction = Dot(axes=1)([user_embedding(user_id), restaurant_embedding(restaurant_id)])\n",
    "\n",
    "# Optional: Add hidden layers for more complex interactions\n",
    "\n",
    "# Prediction Layer\n",
    "prediction = Dense(1, activation=\"sigmoid\")(user_restaurant_interaction)\n",
    "\n",
    "# Model definition\n",
    "model = tf.keras.Model(inputs=[user_id, restaurant_id], outputs=prediction)\n",
    "\n",
    "# Compile the model with optimizer and loss function\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model on user-restaurant interaction data using distributed training \n",
    "# with TensorFlow/Keras on Spark\n",
    "model.fit(..., epochs=...)\n",
    "\n",
    "# Use the trained model to generate recommendations ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, explode, split, array_contains, similarity\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "\n",
    "def recommend_restaurants(user_name, data, lemmatized_column='lemmatized_reviews', k=10):\n",
    "  \"\"\"\n",
    "  Recommends restaurants based on name and lemmatized reviews (CBF)\n",
    "\n",
    "  Args:\n",
    "      user_name: Name of the restaurant for which to recommend similar ones\n",
    "      data: Spark DataFrame containing restaurant data\n",
    "      lemmatized_column: Column containing lemmatized reviews\n",
    "      k: Number of recommendations to return\n",
    "\n",
    "  Returns:\n",
    "      list: List of recommended restaurant names (excluding the user's restaurant)\n",
    "  \"\"\"\n",
    "\n",
    "  # Get user's review vector as a sparse vector\n",
    "  user_review = data.filter(col(\"name\") == user_name).select(col(lemmatized_column)).rdd.flatMap(lambda x: x).first()\n",
    "  user_review_vector = Vectors.sparse(len(data.select(lemmatized_column).first()[0]), user_review)\n",
    "\n",
    "  # Define a UDF for cosine similarity calculation\n",
    "  cosine_similarity_udf = udf(lambda review_vec: review_vec.arraySimilarities(user_review_vector)[0], VectorUDT())\n",
    "\n",
    "  # Add a new column with cosine similarity to all restaurants\n",
    "  data_with_similarity = data.withColumn('similarity', cosine_similarity_udf(col(lemmatized_column)))\n",
    "\n",
    "  # Get top k most similar restaurants (excluding the user's restaurant)\n",
    "  top_k_similar = data_with_similarity.filter(col(\"name\") != user_name).sort(col('similarity').desc()).limit(k)\n",
    "  recommended_restaurants = top_k_similar.select('name').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "  return recommended_restaurants\n",
    "\n",
    "\n",
    "# Example usage (assuming 'restaurants.csv' is your data file)\n",
    "spark = SparkSession.builder.appName(\"RestaurantRecommender\").getOrCreate()\n",
    "data = spark.read.csv(\"restaurants.csv\")  # Replace with your data path\n",
    "\n",
    "user_name = \"Pizza Palace\"  # Replace with the restaurant name for recommendations\n",
    "recommendations = recommend_restaurants(user_name, data)\n",
    "\n",
    "print(f\"Recommendations for {user_name}:\")\n",
    "for restaurant in recommendations:\n",
    "  print(restaurant)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "\n",
    "def cluster_restaurants(data_path, features, k):\n",
    "  \"\"\"\n",
    "  Clusters restaurants using KMeans algorithm in PySpark\n",
    "\n",
    "  Args:\n",
    "      data_path: Path to the CSV file containing restaurant data\n",
    "      features: List of column names to use for clustering (e.g., ['cost', 'rate'])\n",
    "      k: Number of clusters\n",
    "\n",
    "  Returns:\n",
    "      Spark DataFrame: DataFrame containing restaurants with a new column \"cluster\"\n",
    "  \"\"\"\n",
    "\n",
    "  spark = SparkSession.builder.appName(\"RestaurantClustering\").getOrCreate()\n",
    "\n",
    "  # Read data from CSV file\n",
    "  data = spark.read.csv(data_path)\n",
    "\n",
    "  # Create a feature vector assembler\n",
    "  assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "  data_with_features = assembler.transform(data)\n",
    "\n",
    "  # KMeans model\n",
    "  kmeans = KMeans(k=k, seed=1)  # Set random seed for reproducibility\n",
    "\n",
    "  # Fit the model on the features\n",
    "  model = kmeans.fit(data_with_features)\n",
    "\n",
    "  # Predict cluster labels for each restaurant\n",
    "  data_with_cluster = model.transform(data_with_features)\n",
    "\n",
    "  # Return DataFrame with the new \"cluster\" column\n",
    "  return data_with_cluster.select(*data.columns, col(\"prediction\").alias(\"cluster\"))\n",
    "\n",
    "\n",
    "# Example usage\n",
    "data_path = \"restaurants.csv\"\n",
    "features = [\"cost\", \"rate\"]  # Replace with relevant features for clustering\n",
    "k = 3  # Number of clusters\n",
    "\n",
    "clustered_restaurants = cluster_restaurants(data_path, features, k)\n",
    "\n",
    "# Print restaurants with their assigned clusters\n",
    "clustered_restaurants.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow based recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, explode, split\n",
    "\n",
    "def prepare_data(data_path):\n",
    "  \"\"\"\n",
    "  Prepares data for TF-RS model using PySpark\n",
    "\n",
    "  Args:\n",
    "      data_path: Path to the CSV file containing restaurant data\n",
    "\n",
    "  Returns:\n",
    "      tuple: Tuple containing lists of restaurant names and lemmatized reviews\n",
    "  \"\"\"\n",
    "\n",
    "  spark = SparkSession.builder.appName(\"RestaurantRecommender\").getOrCreate()\n",
    "  data = spark.read.csv(data_path)\n",
    "\n",
    "  restaurants = data.select(\"name\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "  def lemmatizer_udf(text):\n",
    "    # ... Implement your lemmatization logic here (same as previous examples)\n",
    "    return processed_text\n",
    "\n",
    "  lemmatizer_udf = udf(lemmatizer_udf, StringType())\n",
    "  reviews = data.withColumn(\"lemmatized_reviews\", lemmatizer_udf(explode(split(col(\"reviews_list\"), \"\\\\|\")))) \\\n",
    "             .select(\"lemmatized_reviews\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "  spark.stop()\n",
    "  return restaurants, reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# Define hyperparameters (adjust as needed)\n",
    "embedding_dim = 128\n",
    "epochs = 10\n",
    "\n",
    "# Get data prepared by PySpark\n",
    "restaurants, reviews = prepare_data(\"restaurant_data.csv\")\n",
    "\n",
    "# Create feature layers\n",
    "restaurant_names = tf.keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "reviews_input = tf.keras.layers.Input(shape=(None,), dtype=\"string\")\n",
    "\n",
    "# Embedding layers for restaurants and reviews\n",
    "restaurant_embedding = Embedding(len(restaurants) + 1, embedding_dim)(restaurant_names)\n",
    "review_embedding = Embedding(max(len(w) for w in reviews) + 1, embedding_dim)(reviews_input)\n",
    "\n",
    "# Average review embeddings to represent a restaurant\n",
    "average_review_embedding = tf.keras.layers.GlobalAveragePooling1D()(review_embedding)\n",
    "\n",
    "# Dot product for similarity between restaurant and average review embedding\n",
    "dot_product = tf.keras.layers.Dot(axes=1)([restaurant_embedding, average_review_embedding])\n",
    "\n",
    "# Recommender model\n",
    "model = tf.keras.Model(inputs=[restaurant_names, reviews_input], outputs=dot_product)\n",
    "\n",
    "# Compile the model (adjust loss function as needed)\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))  # Adjust loss function\n",
    "\n",
    "# Assuming you have prepared training data (restaurant, review, rating) as NumPy arrays\n",
    "# Replace with your training logic\n",
    "restaurant_names_train, reviews_train, labels_train = ...\n",
    "\n",
    "# Train the model on your training data\n",
    "model.fit([restaurant_names_train, reviews_train], labels_train, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_restaurants(user_review, k=5):\n",
    "  \"\"\"\n",
    "  Recommends restaurants based on a user's review\n",
    "\n",
    "  Args:\n",
    "      user_review: User's review as a list of words (lemmatized)\n",
    "      k: Number of recommendations to return\n",
    "\n",
    "  Returns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your data in a pandas DataFrame (replace with your data loading logic)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"restaurant_data.csv\")  # Replace with your data path\n",
    "restaurants = df[\"name\"].tolist()\n",
    "reviews = df[\"lemmatized_reviews\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# Define embedding dimension\n",
    "embedding_dim = 128\n",
    "\n",
    "# Create feature layers\n",
    "restaurant_names = tf.keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "reviews_input = tf.keras.layers.Input(shape=(None,), dtype=\"string\")\n",
    "\n",
    "# Embedding layers for restaurants and reviews\n",
    "restaurant_embedding = Embedding(len(restaurants) + 1, embedding_dim)(restaurant_names)\n",
    "review_embedding = Embedding(max(len(w) for w in reviews) + 1, embedding_dim)(reviews_input)\n",
    "\n",
    "# Average review embeddings to represent a restaurant\n",
    "average_review_embedding = tf.keras.layers.GlobalAveragePooling1D()(review_embedding)\n",
    "\n",
    "# Dot product for similarity between restaurant and average review embedding\n",
    "dot_product = tf.keras.layers.Dot(axes=1)([restaurant_embedding, average_review_embedding])\n",
    "\n",
    "# Recommender model\n",
    "model = tf.keras.Model(inputs=[restaurant_names, reviews_input], outputs=dot_product)\n",
    "\n",
    "# Compile the model (adjust loss function as needed)\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))  # Adjust loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have labeled data for training (restaurant, review, rating)\n",
    "# Replace with your training logic\n",
    "\n",
    "# Train the model on your prepared training data\n",
    "model.fit([restaurant_names_train, reviews_train], labels_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recommend restaurants based on a user's review\n",
    "\n",
    "def recommend_restaurants(user_review, k=5):\n",
    "  \"\"\"\n",
    "  Recommends restaurants based on a user's review\n",
    "\n",
    "  Args:\n",
    "      user_review: User's review as a list of words (lemmatized)\n",
    "      k: Number of recommendations to return\n",
    "\n",
    "  Returns:\n",
    "      list: List of top k recommended restaurant names\n",
    "  \"\"\"\n",
    "\n",
    "  # Convert user review to a tensor\n",
    "  user_review_tensor = tf.convert_to_tensor([user_review])\n",
    "\n",
    "  # Predict similarity scores for all restaurants\n",
    "  predictions = model.predict([tf.fill((1,), restaurants[0]), user_review_tensor])\n",
    "\n",
    "  # Get top k restaurant indexes with highest similarity scores\n",
    "  top_k_indexes = predictions.argsort()[0][-k:][::-1]  # Sort descending and pick top k\n",
    "\n",
    "  # Return recommended restaurant names\n",
    "  return [restaurants[i] for i in top_k_indexes]\n",
    "\n",
    "# Example usage\n",
    "user_review = [\"great\", \"pizza\", \"service\"]  # Replace with user's review\n",
    "recommendations = recommend_restaurants(user_review)\n",
    "\n",
    "print(f\"Recommended restaurants for your review:\")\n",
    "for restaurant in recommendations:\n",
    "  print(restaurant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2024-04-04 - DBFS Example (2)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
